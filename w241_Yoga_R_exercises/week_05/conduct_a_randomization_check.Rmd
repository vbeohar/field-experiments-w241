---
title: "R Notebook"
output: html_notebook
---

Let's work quickly to conduct a covariate balance check to see if a randomization has worked. To do so, we're going to fabricate data and then run tests to whether the tests confirm what we did in the manufactured data. 

```{r}
library(data.table)
set.seed(1) 

rows <- 101 

d <- data.table(id = 1:rows) 
d[ , ':='(treat = sample(c(0,1), .N, replace = TRUE), 
          x1 = rnorm(.N), 
          x2 = rbinom(.N, 2, .25),
          x3 = sample(c("green", "blue", "red"), .N, replace = TRUE), 
          block = factor(sample(c("A", "B", "C"), .N , replace = TRUE)))
  ]

hist(d[, x1])

hist(d[, x2])
```

Note in the lines above that I've been using the `.N` special. This is particular to `data.table` and permits us a variable that is the *number of rows* that meet the current query scope. This is nice, because you can write your work without having to worry that the size of your data is somehow hardcoded to the size that you have in the present moment. Slick! 

# A first test 

Consider what we've got above: In the `treat` variable we've got random assignments to treatment and control, and in the `x1` variable we've got random draws from a normal distribution. Should there be any difference in the mean values of `x1` conditional on being in the treatment or control? 

Of course not! But, is there? 

```{r}
d[ , t.test(x1 ~ treat)]
```

If I were to read the last call aloud, it would read as: 

> From the data.table `d`, select all the rows and conduct a t.test for the difference in means in `x1`, split on the `treat` indicator. 

Of course, I could perform the same test for `x2` right? 

```{r}
d[ ,  t.test(x2 ~ treat)]
```

What would be the appropriate test for the difference in categorical variables, split on the treatment indicator? Look it up, and fill into the next cell. 

```{r}
#head(d)
#d[, I(x3 == 'green')]

d[, t.test(I(x3 == 'green' ~ treat))] # testing green vs. not green


d[, chisq.test(x3, treat)] # is the distribution inside x3 surprisingly different than random given treat. testing for green/red/blue all together, with one test statistic. with the null hypothesis being (green == red == blue). if we reject the null hypothesis, then we say that green <> red <> blue given treat (y variable)


```

# A better test 

As Green and Gerber highlight, if we make three such tests in a row we're going to be subject to the possibility of falsely rejecting the null hypothesis at rates higher than our critical value $\alpha$ should lead us to. This is a problem of *fishing*, or losing control of our Family Wide Error or False Discovery rate. 

How might we control for this? Green and Gerber suggest a clever test! Since the treatment indicator is random, why not try to fit a model that explains a random feature, and compare to just a null model that has no predictive features? 

How would this look? 

## First, fit the simplest model
```{r}
#extra randomization check: 
#regress the teratment variable on all the varibales
#

null_mod <- d[ , lm(treat ~ 1)]
#null_mod <- d[ , lm(treat ~ x1 + x2 + x3 + block)]


# in the null model, what is estimated inside of the intercept? the avg rate of treatment (.48 estimate coeff ~ 50% because of random assignment)

# we should not be able to predict something random from bunch of other things - we should not be able to 
# if we have just an avg indictor or avg + long list of variables - we should not see change in variance inside of the residuals
# we should NOT be able ot increase predictive power by adding all those variables (those variables should not help us explain variance at all)
# every variable will marginally increase predictiveness, with little bit of overfitting
# but F test will penalize with addition of variable because it will increase the threshold. 


```

This is a model that has *only* an intercept, which is indicated in the `1` in the formula call. Note aso that just as we are able to perform relatively simple functions on the column space using `data.table` (e.g. `d[ , mean(x)]`) we can also peform relatively more complex functions -- like a `t.test` or a `lm`. Neato. 

## Now, the more complex model 

```{r}
full_mod <- d[ , lm(treat ~ 1 + x1 + x2 + x3)]
summary(full_mod)
```

# in the results above, we see that the t-stats are all well below 2, which means that there is NO positive difference between our sample data and the null hypothesis (null hypothesis being sample mean is equal to test statistic). so basically we are saying that there is no reasonable probability of obtaining a t-value from -2 to +2 when the null hypothesis is true. 

# so in the regression all coeff t-values are below 2, so none of the coeff are siginificant. so there might be some other combination of x1, x2, x3 that might be siginficant. 



Once we've got both models fit, we can use an **F-test** to evaluate whether the additional model terms were useful in predicting whether someone was in the treatment or the control group. *If these additional model features increase our ability to predict whether someone is in one group or another, it would be evidence of non-randomness!*. 

```{r}
anova_mod <- anova(full_mod, null_mod, test = 'F')
anova_mod


# conclusion: we have no evidence, or at least we say that if the null hypothesis were true (that our randomization has worked well), then we would have generated differences in the variance of outcomes this large or larger than 71% (p-value) of the cases. Thats scant evidence to me that there is problem in our randomization. we didtn explain very much variance by putting in those things (and rightly so since they were generated randomly - and they shouldnt be 
```

Here, we have no evidence to suggest that the additional model terms incrased the accuracy of the model's predictions. Indeed the p-value is nearly 0.5, scant evidence in support of the alternative hypothesis. 

# If you like a challenge 

This is optional, and so less well developed. 

What if you were to block randomize? Suppose that for the "A" block, you don't much care if they get treatment or control; but for the "B" block you'd really like them to get treatment, and for the "C" block you'd really like them to get control. 

Then, you might assign with a slightly more complex scheme. 

```{r}
rm(d)
rows <- 2001                            # set rows
d <- data.table(id = 1:rows)            # create data.table 
d[ ,':='(block = sample(c("A", "B", "C"),
             rows, replace = TRUE)) ]   # make some random block assignments 

blocks <- c("A", "B", "C")              # create a 'blocks' vector to check
                                        # against for the loop that is coming

probs <- list(c(0.5, 0.5),              # create a list of treatment assignment 
              c(0.1, 0.9),              # probabilites. we're going to step 
              c(0.9, 0.1) )             # through these one at a time. 

for(b in 1:3) {                         # start a loop from 1:3
    d[block == blocks[b],               # for each iteration, match against
      ':='(treat = sample(c(0,1), .N,   # the objects 'blocks' in position i
               replace = TRUE,          # take .N samples with probability 
               prob = probs[[b]]),      # drawn from the probs list at indx i
           x1 = rnorm(.N, mean = b),
           x2 = rbinom(.N, 2*b, .25),
           x3 = sample(x = c("green", "blue"), size = .N, replace = T,
               prob = probs[[b]]) )     # notice that we ARE random assigning
      ]                                 # just with different probabilities 
}                                       # and it is working, so a randomization
                                        # check should /not/ generate a positive
                                        # finding
```

Notice as well that in the data creation, there is going to be some covariance between the levels in unit's block and their `x*` values. 

Under this randomization, there will -- of course -- be some relationship between the x values and the treatment indicators. Just look: 

```{r}
d[ , t.test(x1 ~ treat)]
```
What is causing this? Can you point to it closely? To do so, write three tests for differences in `x1` by treatment, but for each of them, conduct it within a block. 

Here's a hint: You don't want to select all the rows; instead, you want to select the rows, block-by-block. 

```{r}

```

How might you similarily conduct your `anova` test within blocks? 

```{r}

```

